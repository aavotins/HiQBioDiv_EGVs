[["index.html", "High-resolution ecogeographical variables for species distribution modelling describing Latvia, 2024 Preface About this material Outline", " High-resolution ecogeographical variables for species distribution modelling describing Latvia, 2024 Andris Avotiņš 2025-10-07 Preface Welcome! This book documents the geodata and processing workflows used to create ecogeographical variables (EGVs) for species distribution modelling in Latvia (2024). This material has been developed to present the results of three projects implemented at the University of Latvia, which are deeply rooted in species distribution modeling, and, more importantly, to demonstrate and explain the work process and decisions made in order to ensure their repeatability and reproducibility. These projects are: The project “Preparation of a geospatial data layer covering existing protected areas for the implementation of the EU Biodiversity Strategy 2030” (No. 1-08/73/2023), funded by the Latvian Environmental Protection Fund Administration ; Scientific research service project commissioned by AS “Latvijas valsts meži” (Latvian State Forests) “Improvement of the monitoring of the northern goshawk Accipiter gentilis and creation of a spatial model of habitat suitability” (Latvian State Forests document No. 5-5.5.1_000r_101_23_27_6); State research program “Development of research specified in the Biodiversity Priority Action Program” project “High-resolution quantification of biodiversity for nature conservation and management: HiQBioDiv” (VPP-VARAM-DABA-2024/1-0002). The material was developed in R using {bookdown}. The data processing and analysis described in the content was mainly performed in R, and one of the main reasons for creating this material was to transfer the information necessary for reproducing the work using verified command lines. A desirable side effect is to promote openness and reproducibility in scientific practice and practical science. Repo: aavotins/HiQBioDiv_EGVs Cite as needed using book/book.bib. About this material This material is not: an introduction to R or other programming languages. On the contrary, it will be most useful to those who already understand how to use command lines. However, it will also be informative for other users regarding the approaches used; a tutorial on geoprocessing. This material summarizes the approaches that, at the time of its development, were known to the authors as the most effective (in terms of processing time, RAM and hard disk space, performance guarantees, and reliability), but they are certainly not the only ones possible; copy/paste ready product. Although the use and publication of command lines tends to be intended for these purposes, in a situation where large amounts of data and, at least in part, restricted access data are used for the work, this is simply not possible. However, by ensuring data availability and placement in accordance with the file structure of this project (availabe at root/Data or by forking template repository), the command lines will be repeatable without changes and will produce the same results. This material has been prepared to provide a reproducible workflow, describing the decisions made and solutions implemented in the preparation of ecogeographical variables for species distribution (habitat suitability) modeling for biodiversity conservation planning. To a lesser extent, it also serves to demonstrate the results and ensure their accessibility. For the most part, this material consists of: explanatory text, which is recognizable as text; command lines, which are hidden by default to make the text easier to read. The locations of the command lines can be identified by the “|&gt; Code” visible on the left side of the page, just below this paragraph. Clicking on it will open the code area, where the text on a gray background is command lines, for example: Code object=function(arguments1,arguments2, path=&quot;./path/file/tree/object.extension&quot;) # comment In the example above, the first line creates an object (“object”) that is the result of a function (“function()”). The function has three arguments (“arguments1”, “arguments2” and “path”) separated by commas (as with all function arguments in R). The third argument is the path in the file tree (it is on a new line but is a continuation of the function on the previous line, because the parentheses are not closed), which is indicated by an equal sign (and quotation marks) followed by this path (note the beginning “./”, which indicates a relative path - the location in the file tree is relative to the project location). The second line of the example above is a comment - everything after “#” is a comment. Anything in a command line before “#” must be an executable function or object. A comment can contain anything and be on the same line as an executable function (at the end of it). Command lines are the most important part of this material for reproducibility. However, the person using them must ensure the availability of input data and maintain correct paths in the file tree. Command lines can also be found in text, for example, # comment as a command line in text. Sometimes I will refer to R packages in the text, I will put them in curly brackets, for example, {package}. graphics - various diagrams that describe the workflow or data characteristics; links to other resources, especially to higher-level products and results created within the project, but also to input data, if it is publicly available. The results are intended for practical use. Within reason, the material describes all data sets used and provides metadata related to ensuring reproducibility. Since not all data sets are freely available, they are not published as such, but in all cases information is provided on how they were obtained for the development of this project. Outline Terminology and acronyms Utilities Template files Raw geodata Geodata products Ecogeographical variables Data accessibility "],["Ch01.html", "1 Terminology and acronyms", " 1 Terminology and acronyms Athough all georeferenced data can be considered geodata, in this material we use the following terms in the order listed below in our workflows: raw geodata - considered as raw data obtained for a harmonised description of the environment. This may include tables with coordinates, raster or vector data. It can be anything that has been or can be used to create ecogeographical variables, with or without slight processing. geodata product - processed raw geodata that have undegone heavy modifications, e.g.  spatial overlays and combinations of different sets of raw geodata, and are used as input data. In this document, geodata products are categorical raster layers that match the CRS and the pixel locations of input data. When split by categories, they become input data. The processing step of creating geodata products is necessary when decisions about the order of spatial overlays are important. For example, in a high-resolution pixel, there can only be water or forest, if the edge between water and forest need to be calculated. input data or input layers - very-high resolution (multiple times higher than that used for ecogeographical variables) raster data that are the direct input for the creation of most of the ecogeographical variables. The creation of such layers is particularly useful alongside geodata products, as dealing with border misalignment or decisions regarding the order of spatial overlays, as well as simple geoprocessing, is much faster with raster data. ecogeographical variables (EGVs) - this is the final product of the workflow describing environment for statistical analysis (e.g. species distribution modelling). They are suitable also for publishing due to standadisation of the values. In other words, these are standardised landscape ecological variables in the for of high-resolution raster layers (we use 1 ha cells). Each layer contains values representing the environment within the cell footprint or a summary of focal neighbours. In our case, each layer is of quantitative data describing a natural quantity (e.g. timber volume, mean annual temperature), or quantified information of categories (e.g. the fraction of class’s area in an analysis cell or some neighbourhood, the number of pixels creating an edge of a certain class or between two classes in the analysis cell or some neighbourhood). The values of each layer are standardised - from every cells value layers mean is subtracted and then every cells value is divided by layers root mean square error. Therefore, the values are more suitable for modelling, and the layers can be made publicly available as they do not directly provide exact sensitive information. In this material, we use the term species distribution modelling as a more used term, that is synonymous with ecological niche analysis and ecological niche modelling. Acronyms: CRS - coordinate reference system EGV - ecogeoraphical variables SDM - species distribution modelling SDMs - species distribution models "],["Ch02.html", "2 Utilities 2.1 R package egvtools 2.2 Other utility functions", " 2 Utilities This chapter provides a brief description of the utility functions used in this material. Most of these functions are packaged in the R package {egvtools}, which was created specifically for this work. 2.1 R package egvtools {egvtools} provides a coherent set of wrappers and utilities that facilitate the reproducible and efficient creation of large-scale EGVs on real datasets. The package relies on robust building blocks — {terra}, {sf}, {sfarrow}, {exactextractr} and {whitebox} — and standardises input/output, naming conventions and multi-scale zonal statistics, ensuring that the pipelines are repeatable across machines and projects. The package was developed for the project ‘HiQBioDiv: High-resolution quantification of biodiversity for conservation and management’, which was funded by the Latvian Council of Science (Ref. No. VPP-VARAM-DABA-2024/1-0002), to simplify our work and to facilitate the reproduction of our results. Five of the functions are strictly for replication, while others are useful for a wider audience. Package can be installed from GitHub with: Code # install.packages(&quot;pak&quot;) pak::pak(&quot;aavotins/egvtools&quot;) or obtained as a Docker container with all the necessary system and software dependencies. 2.1.1 Reproduction only functions These functions are small wrappers, that helps to recreate our working environments - template files and their locations in the file tree. These functions are: download_raster_templates() — fetch template rasters from Zenodo repository and place them in user specified location on disk, or by default - the one we used. By default this functions links to version 2.0.0 of the dataset; download_vector_templates() - fetch template vector grids/points from Zenodo repository and place them in user specified location on disk, or by default - the one we used. By default this functions links to version 1.0.1 of the dataset. 2.1.2 General purpose functions Each of those functions are small workflows themselves, that can be combined into larger workflows and used more widely, than for Latvia. tile_vector_grid() — tile template (vector) grid for chunked processing. The function internally is linked to our file naming convention. As long as it is maintained, function can be used to create tiled grid from any {sfarrow} parquet grid file; tiled_buffers() — precompute buffered tiles for multiple radii around points. The function internally is linked to our file naming convention. As long as it is maintained, function can be used to create tiled polygons with buffers around points from any {sfarrow} parquet grid file. There are three buffering modes: dense (buffers the best-matching pts100*.parquet (prefers pts100_sauzeme.parquet) for each tile by radii_dense (default: 500, 1250, 3000, 10000 m ensuring that every analysis grid cell has desired buffer. Computationally heavy in the following workflows), sparse (uses a file to radius mapping and is highly generalizable), and specified (the same as sparse, but with one single point file). In our workflows we used the sparse mode with default mapping; create_backgrounds() — a wrapper around terra::ifel() to build consistent background rasters. This function better guards coordinate reference system and how it is stored, while also guarding spatial cover, resolution, coordinate reference system, exact pixel matching, etc. Creation of layers with default background values is faster than recreating them several times in workflows preparing EGVs; polygon2input() — rasterize polygons to input layers. Handles only polygon data, other geometry types need to buffered. Rasterizes polygon/multipolygon sf data to a raster aligned to a template GeoTIFF. Rasterization targets a raster::RasterLayer built from the template (so grids normally match). Projection is optional (project_mode). Missing values are counted only over valid template cells. User may optionally restrict the result with a raster mask (restrict_to) using numeric values or bracketed range strings (e.g., “(0,5]”, “[10,)”). Remaining NA cells can be filled by covering with a background raster (background_raster) or a constant (background_value). For large rasters, heavy steps (projection/mask/cover) can stream to disk via terra_todisk=TRUE. input2egv() — normalize/align a fine-resolution input raster to a (coarser) EGV template, optionally cover missing values and/or fill gaps (IDW via Whitebox), and write the result to disk. Designed for large runs: fast gap counting (inside template footprint only), optional filling, tuned GDAL write options, and controlled terra memory/temp behavior. downscale2egv() — downscale coarse rasters to a template grid (CRS, resolution, extent), masks to the template footprint, and optionally: (1) fills NoData gaps using WhiteboxTools’ IDW-based fill_missing_data, and (2) applies IDW smoothing to reduce blockiness from low-resolution inputs. distance2egv() — computes Euclidean distance (in map units) from cells matching a set of class values in an input raster to all cells of an EGV template grid, then writes a Float32 GeoTIFF aligned to the template. Designed to work with rasters produced by polygon2input(). landscape_function() — computes a {landscapemetrics} metric (default “lsm_l_shdi”), optionally with extra lm_args, that yields one value per zone and per input layer. Runs tile-by-tile (by tile_field), writes per-tile rasters, merges to final per-layer GeoTIFF(s), then performs gap analysis (NA count within the template footprint and optional maximum gap width) and optional IDW gap filling via WhiteboxTools. Returns a compact data.frame with per-layer stats and timing. radius_function() — extracts summary statistics from raster layers using buffered polygon zones of multiple radii and rasterizes them onto a common template grid. 2.2 Other utility functions Other handy functions repeatedly used, not included in {egvtools} are stored in UtilityFunctions.R file, located next to .Rproj file. ensure_multipolygons() - rather agressive function to create MULTIPOLYGON geometries from GEOMETRYCOLLECTION Code ensure_multipolygons &lt;- function(X) { library(sf) library(gdalUtilities) tmp1 &lt;- tempfile(fileext = &quot;.gpkg&quot;) tmp2 &lt;- tempfile(fileext = &quot;.gpkg&quot;) st_write(X, tmp1) ogr2ogr(tmp1, tmp2, f = &quot;GPKG&quot;, nlt = &quot;MULTIPOLYGON&quot;) Y &lt;- st_read(tmp2) st_sf(st_drop_geometry(X), geom = st_geometry(Y)) } "],["Ch03.html", "3 Templates and utilities 3.1 Vector data 3.2 Raster data", " 3 Templates and utilities This chapter defines template files. They define the analysis space and ensure harmonisation of georeferenced data creation, and facilitate connection with other Latvian geodata. 3.1 Vector data Baseline template (or reference) vector grid and point files are publically available at HiQBioDiv’s Zenodo repository. Command lines and data used to create these files are documented at the HiQBioDiv main code repository’s file. The easiest way to obtain these files is to run determined function download_vector_templates() from {egvtools}. Code download_vector_templates( url = &quot;https://zenodo.org/api/records/14277114/files-archive&quot;, grid_dir = &quot;./Templates/TemplateGrids&quot;, points_dir = &quot;./Templates/TemplateGridPoints&quot;, gpkg_dir = &quot;./Templates&quot;, overwrite = FALSE, quiet = FALSE ) Once template vector data are downloaded and unarchived, they need to be tiled: Analysis grid is tiled in tks50km pages Code tile_vector_grid( grid_path = &quot;./Templates/TemplateGrids/tikls100_sauzeme.parquet&quot;, out_dir = &quot;./Templates/TemplateGrids/lapas&quot;, tile_field = &quot;tks50km&quot;, chunk_size = 50000L, overwrite = FALSE, quiet = FALSE ) Point files are tiled and buffered. In workflows creating EGVs described in this document, we used “sparse” grid: 500m buffers around every 100m grids center; 1250m buffers around every 100m grids center; 3000m buffers around every 300m grids center (to speed up neighbourhood analysis ~9 times, while loosing &lt;0.001% of precission); 10000m buffers around every 1000m grids center (to speed up neighbourhood analysis ~100 times, while loosing &lt;0.001% of precission) Code tiled_buffers( in_dir = &quot;./Templates/TemplateGridPoints&quot;, out_dir = &quot;./Templates/TemplateGridPoints/lapas&quot;, buffer_mode = &quot;sparse&quot;, mapping_sparse = list(pts100_sauzeme.parquet = c(500, 1250), pts300_sauzeme.parquet = 3000, pts1000_sauzeme.parquet = 10000), split_field = &quot;tks50km&quot;, n_workers = 4, os_type = NULL, future_max_mem_gb = 4, overwrite = FALSE, quiet = FALSE ) 3.2 Raster data Baseline template (or reference) raster grid and point files are publically available at HiQBioDiv’s Zenodo repository. Command lines and data used to create these files are documented at the HiQBioDiv main code repository’s file. The easiest way to obtain these files is to run determined function download_raster_templates() from {egvtools}. Code download_raster_templates( url = &quot;https://zenodo.org/api/records/14497070/files-archive&quot;, out_dir = &quot;./Templates/TemplateRasters&quot;, overwrite = FALSE, quiet = FALSE ) "],["Ch04.html", "4 Raw geodata 4.1 State Forest Services State Forest Registry (Ch04.01)", " 4 Raw geodata This chapter describes raw geodata used and the preliminary processing conducted on them. 4.1 State Forest Services State Forest Registry (Ch04.01) krā "],["Ch05.html", "5 Geodata products", " 5 Geodata products some raw data need extensive processing before EGVs can be created "],["Ch06.html", "6 Ecogeographical variables", " 6 Ecogeographical variables Creation procedures of every EGV. "],["Ch07.html", "7 Data accessibility", " 7 Data accessibility This chapter provides access to EGVs described in previous parts "]]
