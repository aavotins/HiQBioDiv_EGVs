[["index.html", "High-resolution ecogeographical variables for species distribution modelling describing Latvia, 2024 Preface About this material Outline", " High-resolution ecogeographical variables for species distribution modelling describing Latvia, 2024 Andris Avotiņš 2025-10-09 Preface Welcome! This book documents the geodata and processing workflows used to create ecogeographical variables (EGVs) for species distribution modelling in Latvia (2024). This material has been developed to present the results of three projects implemented at the University of Latvia, which are deeply rooted in species distribution modeling, and, more importantly, to demonstrate and explain the work process and decisions made in order to ensure their repeatability and reproducibility. These projects are: The project “Preparation of a geospatial data layer covering existing protected areas for the implementation of the EU Biodiversity Strategy 2030” (No. 1-08/73/2023), funded by the Latvian Environmental Protection Fund Administration ; Scientific research service project commissioned by AS “Latvijas valsts meži” (Latvian State Forests) “Improvement of the monitoring of the northern goshawk Accipiter gentilis and creation of a spatial model of habitat suitability” (Latvian State Forests document No. 5-5.5.1_000r_101_23_27_6); State research program “Development of research specified in the Biodiversity Priority Action Program” project “High-resolution quantification of biodiversity for nature conservation and management: HiQBioDiv” (VPP-VARAM-DABA-2024/1-0002). The material was developed in R using {bookdown}. The data processing and analysis described in the content was mainly performed in R, and one of the main reasons for creating this material was to transfer the information necessary for reproducing the work using verified command lines. A desirable side effect is to promote openness and reproducibility in scientific practice and practical science. Repo: aavotins/HiQBioDiv_EGVs Cite as needed using book/book.bib. About this material This material is not: an introduction to R or other programming languages. On the contrary, it will be most useful to those who already understand how to use command lines. However, it will also be informative for other users regarding the approaches used; a tutorial on geoprocessing. This material summarizes the approaches that, at the time of its development, were known to the authors as the most effective (in terms of processing time, RAM and hard disk space, performance guarantees, and reliability), but they are certainly not the only ones possible; copy/paste ready product. Although the use and publication of command lines tends to be intended for these purposes, in a situation where large amounts of data and, at least in part, restricted access data are used for the work, this is simply not possible. However, by ensuring data availability and placement in accordance with the file structure of this project (availabe at root/Data or by forking template repository), the command lines will be repeatable without changes and will produce the same results. This material has been prepared to provide a reproducible workflow, describing the decisions made and solutions implemented in the preparation of ecogeographical variables for species distribution (habitat suitability) modeling for biodiversity conservation planning. To a lesser extent, it also serves to demonstrate the results and ensure their accessibility. For the most part, this material consists of: explanatory text, which is recognizable as text; command lines, which are hidden by default to make the text easier to read. The locations of the command lines can be identified by the “|&gt; Code” visible on the left side of the page, just below this paragraph. Clicking on it will open the code area, where the text on a gray background is command lines, for example: Code object=function(arguments1,arguments2, path=&quot;./path/file/tree/object.extension&quot;) # comment In the example above, the first line creates an object (“object”) that is the result of a function (“function()”). The function has three arguments (“arguments1”, “arguments2” and “path”) separated by commas (as with all function arguments in R). The third argument is the path in the file tree (it is on a new line but is a continuation of the function on the previous line, because the parentheses are not closed), which is indicated by an equal sign (and quotation marks) followed by this path (note the beginning “./”, which indicates a relative path - the location in the file tree is relative to the project location). The second line of the example above is a comment - everything after “#” is a comment. Anything in a command line before “#” must be an executable function or object. A comment can contain anything and be on the same line as an executable function (at the end of it). Command lines are the most important part of this material for reproducibility. However, the person using them must ensure the availability of input data and maintain correct paths in the file tree. Command lines can also be found in text, for example, # comment as a command line in text. Sometimes I will refer to R packages in the text, I will put them in curly brackets, for example, {package}. graphics - various diagrams that describe the workflow or data characteristics; links to other resources, especially to higher-level products and results created within the project, but also to input data, if it is publicly available. The results are intended for practical use. Within reason, the material describes all data sets used and provides metadata related to ensuring reproducibility. Since not all data sets are freely available, they are not published as such, but in all cases information is provided on how they were obtained for the development of this project. Outline Terminology and acronyms Utilities Template files Raw geodata Geodata products Ecogeographical variables Data accessibility "],["Ch01.html", "1 Terminology and acronyms", " 1 Terminology and acronyms Athough all georeferenced data can be considered geodata, in this material we use the following terms in the order listed below in our workflows: raw geodata - considered as raw data obtained for a harmonised description of the environment. This may include tables with coordinates, raster or vector data. It can be anything that has been or can be used to create ecogeographical variables, with or without slight processing. geodata product - processed raw geodata that have undegone heavy modifications, e.g.  spatial overlays and combinations of different sets of raw geodata, and are used as input data. In this document, geodata products are categorical raster layers that match the CRS and the pixel locations of input data. When split by categories, they become input data. The processing step of creating geodata products is necessary when decisions about the order of spatial overlays are important. For example, in a high-resolution pixel, there can only be water or forest, if the edge between water and forest need to be calculated. input data or input layers - very-high resolution (multiple times higher than that used for ecogeographical variables) raster data that are the direct input for the creation of most of the ecogeographical variables. The creation of such layers is particularly useful alongside geodata products, as dealing with border misalignment or decisions regarding the order of spatial overlays, as well as simple geoprocessing, is much faster with raster data. ecogeographical variables (EGVs) - this is the final product of the workflow describing environment for statistical analysis (e.g. species distribution modelling). They are suitable also for publishing due to standadisation of the values. In other words, these are standardised landscape ecological variables in the for of high-resolution raster layers (we use 1 ha cells). Each layer contains values representing the environment within the cell footprint or a summary of focal neighbours. In our case, each layer is of quantitative data describing a natural quantity (e.g. timber volume, mean annual temperature), or quantified information of categories (e.g. the fraction of class’s area in an analysis cell or some neighbourhood, the number of pixels creating an edge of a certain class or between two classes in the analysis cell or some neighbourhood). The values of each layer are standardised - from every cells value layers mean is subtracted and then every cells value is divided by layers root mean square error. Therefore, the values are more suitable for modelling, and the layers can be made publicly available as they do not directly provide exact sensitive information. In this material, we use the term species distribution modelling as a more used term, that is synonymous with ecological niche analysis and ecological niche modelling. Acronyms: CRS - coordinate reference system EGV - ecogeoraphical variables SDM - species distribution modelling SDMs - species distribution models NDMI - normalized difference moisture index NDVI - normalized difference vegetation index NDWI - normalized difference water index "],["Ch02.html", "2 Utilities 2.1 R package egvtools 2.2 Other utility functions", " 2 Utilities This chapter provides a brief description of the utility functions used in this material. Most of these functions are packaged in the R package {egvtools}, which was created specifically for this work. 2.1 R package egvtools {egvtools} provides a coherent set of wrappers and utilities that facilitate the reproducible and efficient creation of large-scale EGVs on real datasets. The package relies on robust building blocks — {terra}, {sf}, {sfarrow}, {exactextractr} and {whitebox} — and standardises input/output, naming conventions and multi-scale zonal statistics, ensuring that the pipelines are repeatable across machines and projects. The package was developed for the project ‘HiQBioDiv: High-resolution quantification of biodiversity for conservation and management’, which was funded by the Latvian Council of Science (Ref. No. VPP-VARAM-DABA-2024/1-0002), to simplify our work and to facilitate the reproduction of our results. Five of the functions are strictly for replication, while others are useful for a wider audience. Package can be installed from GitHub with: Code # install.packages(&quot;pak&quot;) pak::pak(&quot;aavotins/egvtools&quot;) or obtained as a Docker container with all the necessary system and software dependencies. 2.1.1 Reproduction only functions These functions are small wrappers, that helps to recreate our working environments - template files and their locations in the file tree. These functions are: download_raster_templates() — fetch template rasters from Zenodo repository and place them in user specified location on disk, or by default - the one we used. By default this functions links to version 2.0.0 of the dataset; download_vector_templates() - fetch template vector grids/points from Zenodo repository and place them in user specified location on disk, or by default - the one we used. By default this functions links to version 1.0.1 of the dataset. 2.1.2 General purpose functions Each of those functions are small workflows themselves, that can be combined into larger workflows and used more widely, than for Latvia. tile_vector_grid() — tile template (vector) grid for chunked processing. The function internally is linked to our file naming convention. As long as it is maintained, function can be used to create tiled grid from any {sfarrow} parquet grid file; tiled_buffers() — precompute buffered tiles for multiple radii around points. The function internally is linked to our file naming convention. As long as it is maintained, function can be used to create tiled polygons with buffers around points from any {sfarrow} parquet grid file. There are three buffering modes: dense (buffers the best-matching pts100*.parquet (prefers pts100_sauzeme.parquet) for each tile by radii_dense (default: 500, 1250, 3000, 10000 m ensuring that every analysis grid cell has desired buffer. Computationally heavy in the following workflows), sparse (uses a file to radius mapping and is highly generalizable), and specified (the same as sparse, but with one single point file). In our workflows we used the sparse mode with default mapping; create_backgrounds() — a wrapper around terra::ifel() to build consistent background rasters. This function better guards coordinate reference system and how it is stored, while also guarding spatial cover, resolution, coordinate reference system, exact pixel matching, etc. Creation of layers with default background values is faster than recreating them several times in workflows preparing EGVs; polygon2input() — rasterize polygons to input layers. Handles only polygon data, other geometry types need to buffered. Rasterizes polygon/multipolygon sf data to a raster aligned to a template GeoTIFF. Rasterization targets a raster::RasterLayer built from the template (so grids normally match). Projection is optional (project_mode). Missing values are counted only over valid template cells. User may optionally restrict the result with a raster mask (restrict_to) using numeric values or bracketed range strings (e.g., “(0,5]”, “[10,)”). Remaining NA cells can be filled by covering with a background raster (background_raster) or a constant (background_value). For large rasters, heavy steps (projection/mask/cover) can stream to disk via terra_todisk=TRUE. input2egv() — normalize/align a fine-resolution input raster to a (coarser) EGV template, optionally cover missing values and/or fill gaps (IDW via Whitebox), and write the result to disk. Designed for large runs: fast gap counting (inside template footprint only), optional filling, tuned GDAL write options, and controlled terra memory/temp behavior. downscale2egv() — downscale coarse rasters to a template grid (CRS, resolution, extent), masks to the template footprint, and optionally: (1) fills NoData gaps using WhiteboxTools’ IDW-based fill_missing_data, and (2) applies IDW smoothing to reduce blockiness from low-resolution inputs. distance2egv() — computes Euclidean distance (in map units) from cells matching a set of class values in an input raster to all cells of an EGV template grid, then writes a Float32 GeoTIFF aligned to the template. Designed to work with rasters produced by polygon2input(). landscape_function() — computes a {landscapemetrics} metric (default “lsm_l_shdi”), optionally with extra lm_args, that yields one value per zone and per input layer. Runs tile-by-tile (by tile_field), writes per-tile rasters, merges to final per-layer GeoTIFF(s), then performs gap analysis (NA count within the template footprint and optional maximum gap width) and optional IDW gap filling via WhiteboxTools. Returns a compact data.frame with per-layer stats and timing. radius_function() — extracts summary statistics from raster layers using buffered polygon zones of multiple radii and rasterizes them onto a common template grid. 2.2 Other utility functions Other handy functions repeatedly used, not included in {egvtools} are stored in egvs02.02_UtilityFunctions.R file, located in Data/RScipts_final. ensure_multipolygons() - rather agressive function to create MULTIPOLYGON geometries from GEOMETRYCOLLECTION Code if(!require(sf)) {install.packages(&quot;sf&quot;); require(sf)} if(!require(gdalUtilities)) {install.packages(&quot;gdalUtilities&quot;); require(gdalUtilities)} ensure_multipolygons &lt;- function(X) { library(sf) library(gdalUtilities) tmp1 &lt;- tempfile(fileext = &quot;.gpkg&quot;) tmp2 &lt;- tempfile(fileext = &quot;.gpkg&quot;) st_write(X, tmp1) ogr2ogr(tmp1, tmp2, f = &quot;GPKG&quot;, nlt = &quot;MULTIPOLYGON&quot;) Y &lt;- st_read(tmp2) st_sf(st_drop_geometry(X), geom = st_geometry(Y)) } "],["Ch03.html", "3 Templates and utilities 3.1 Vector data 3.2 Raster data", " 3 Templates and utilities This chapter defines template files. They define the analysis space and ensure harmonisation of georeferenced data creation, and facilitate connection with other Latvian geodata. 3.1 Vector data Baseline template (or reference) vector grid and point files are publically available at HiQBioDiv’s Zenodo repository. Command lines and data used to create these files are documented at the HiQBioDiv main code repository’s file. The easiest way to obtain these files is to run determined function download_vector_templates() from {egvtools}. Code if(!require(egvtools)) {remotes::install_github(&quot;aavotins/egvtools&quot;); require(egvtools)} download_vector_templates( url = &quot;https://zenodo.org/api/records/14277114/files-archive&quot;, grid_dir = &quot;./Templates/TemplateGrids&quot;, points_dir = &quot;./Templates/TemplateGridPoints&quot;, gpkg_dir = &quot;./Templates&quot;, overwrite = FALSE, quiet = FALSE ) Once template vector data are downloaded and unarchived, they need to be tiled: Analysis grid is tiled in tks50km pages Code if(!require(egvtools)) {remotes::install_github(&quot;aavotins/egvtools&quot;); require(egvtools)} tile_vector_grid( grid_path = &quot;./Templates/TemplateGrids/tikls100_sauzeme.parquet&quot;, out_dir = &quot;./Templates/TemplateGrids/tiles&quot;, tile_field = &quot;tks50km&quot;, chunk_size = 50000L, overwrite = FALSE, quiet = FALSE ) Expect to see warning: This is an initial implementation of Parquet/Feather file support and geo metadata. This is tracking version 0.1.0 of the metadata (https://github.com/geopandas/geo-arrow-spec). This metadata specification may change and does not yet make stability promises. We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files. Point files are tiled and buffered. In workflows creating EGVs described in this document, we used “sparse” grid: 500m buffers around every 100m grids center; 1250m buffers around every 100m grids center; 3000m buffers around every 300m grids center (to speed up neighbourhood analysis ~9 times, while loosing &lt;0.001% of precission); 10000m buffers around every 1000m grids center (to speed up neighbourhood analysis ~100 times, while loosing &lt;0.001% of precission) Code if(!require(egvtools)) {remotes::install_github(&quot;aavotins/egvtools&quot;); require(egvtools)} tiled_buffers( in_dir = &quot;./Templates/TemplateGridPoints&quot;, out_dir = &quot;./Templates/TemplateGridPoints/tiles&quot;, buffer_mode = &quot;sparse&quot;, mapping_sparse = list(pts100_sauzeme.parquet = c(500, 1250), pts300_sauzeme.parquet = 3000, pts1000_sauzeme.parquet = 10000), split_field = &quot;tks50km&quot;, n_workers = 4, os_type = NULL, future_max_mem_gb = 4, overwrite = FALSE, quiet = FALSE ) Expect to see warning: This is an initial implementation of Parquet/Feather file support and geo metadata. This is tracking version 0.1.0 of the metadata (https://github.com/geopandas/geo-arrow-spec). This metadata specification may change and does not yet make stability promises. We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files. Apperance of file pts300_r3000_NA.parquet, i.e. without a tile number, is expected, due to slight mismatch of 300 m grid with the 50 km one. 3.2 Raster data Baseline template (or reference) raster grid and point files are publically available at HiQBioDiv’s Zenodo repository. Command lines and data used to create these files are documented at the HiQBioDiv main code repository’s file. The easiest way to obtain these files is to run determined function download_raster_templates() from {egvtools}. Code if(!require(egvtools)) {remotes::install_github(&quot;aavotins/egvtools&quot;); require(egvtools)} download_raster_templates( url = &quot;https://zenodo.org/api/records/14497070/files-archive&quot;, out_dir = &quot;./Templates/TemplateRasters&quot;, overwrite = FALSE, quiet = FALSE ) "],["Ch04.html", "4 Raw geodata 4.1 State Forest Service’s State Forest Registry 4.2 Rural Support Service’s information on declared fields 4.3 Melioration Cadaster 4.4 TopographicMap 4.5 Corine Land Cover 2018 4.6 Publicly available LVM data 4.7 Soil data 4.8 Dynamic World data 4.9 The Global Forest Watch 4.10 Palsar 4.11 CHELSA v2.1 4.12 HydroClim data 4.13 Sentinel-2 indices 4.14 Waste and garbage disposal sites, landfills 4.15 Digital elevation/terrain models 4.16 Latvian Exclusive Economic Zone polygon 4.17 Nature Conservation Agency’s data", " 4 Raw geodata This chapter describes raw geodata used and the preliminary processing conducted on them. 4.1 State Forest Service’s State Forest Registry krā 4.2 Rural Support Service’s information on declared fields krā 4.3 Melioration Cadaster krā 4.4 TopographicMap krā 4.5 Corine Land Cover 2018 krā 4.6 Publicly available LVM data krā 4.7 Soil data krā 4.7.1 Soil chemistry krā 4.7.2 Soil texture Europe krā 4.7.3 Soil texture LIZ krā 4.7.4 Soil texture Quaternary krā 4.7.5 Organic soils SILAVA krā 4.7.6 Organic soils LU krā 4.8 Dynamic World data krā 4.9 The Global Forest Watch krā 4.10 Palsar krā 4.11 CHELSA v2.1 krā (Karger et al., 2017) https://chelsa-climate.org/ https://chelsa-climate.org/wp-admin/download-page/CHELSA_tech_specification_V2.pdf 4.12 HydroClim data krā (Domisch et al., 2015) https://zenodo.org/records/5089529 https://datadryad.org/dataset/doi:10.5061/dryad.dv920 4.13 Sentinel-2 indices The European Space Agency (ESA) Copernicus program’s Sentinel-2 mission is a constellation of two (three since 09/05/2024) identical satellites orbiting in the same orbit. The first satellite, Sentinel-2A, entered its orbit and underwent calibration tests on 2015-06-23, the second (Sentinel-2B) on 2017-03-07, with the first images available earlier. Each satellite captures high-resolution images (from 10 m (at the equator) pixel resolution) in 13 spectral channels with a return time of up to 5 days (more frequently closer to the poles) (https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2). The data from this mission is freely available, including on the Google Earth Engine platform (Gorelick et al., 2017) for various large-scale pre-processing and analysis. We use the harmonized Level-2A (https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED#description) product, applying a cloud mask that includes not only cloud filtering but also shadow filtering, so that for each filtered (cloud and seasonal - from April to October and from 2020 to 2024) to calculate the normalized difference vegetation index (NDVI), the normalized difference moisture index (NDMI), and the normalized difference water index (NDWI) as well as various metrics. A replication script can be used to prepare the data. To use this script, you need a GEE account and project and sufficient space on Google Drive. When executing the command lines, the following files will be offered for download: NDVI_median-ST-[runtag, 20250820 by default] - NDVI short-term median (2020-2024) of annual medians (April to October) NDVI_p25-ST-[runtag, 20250820 by default] - NDVI short-term median (2020-2024) of annual 25th percentiles (April to October) NDVI_p75-ST-[runtag, 20250820 by default]- NDVI short-term median (2020-2024) of annual 75th percentiles (April to October) NDVI_iqr-ST-[runtag, 20250820 by default] - NDVI short-term median (2020-2024) of inter-quartile ranges (April to October) NDVI_median-LY-[runtag, 20250820 by default] - NDVI last-years (2024) median (April to October) NDMI_median-ST-[runtag, 20250820 by default] - NDMI short-term median (2020-2024) of annual medians (April to October) NDMI_p25-ST-[runtag, 20250820 by default] - NDMI short-term median (2020-2024) of annual 25th percentiles (April to October) NDMI_p75-ST-[runtag, 20250820 by default] - NDMI short-term median (2020-2024) of annual 75th percentiles (April to October) NDMI_iqr-ST-[runtag, 20250820 by default] - NDMI short-term median (2020-2024) of inter-quartile ranges (April to October) NDMI_median-LY-[runtag, 20250820 by default] - NDMI last-years (2024) median (April to October) NDWI_median-ST-[runtag, 20250820 by default] - NDMI short-term median (2020-2024) of annual medians (April to October) NDWI_p25-ST-[runtag, 20250820 by default] - NDWI short-term median (2020-2024) of annual 25th percentiles (April to October) NDWI_p75-ST-[runtag, 20250820 by default] - NDWI short-term median (2020-2024) of annual 75th percentiles (April to October) NDWI_iqr-ST-[runtag, 20250820 by default] - NDWI short-term median (2020-2024) of inter-quartile ranges (April to October) NDWI_median-LY-[runtag, 20250820 by default] - NDWI last-years (2024) median (April to October) After executing the command line and preparing the results in Google Drive, it can be seen that each layer covering the whole of Latvia is divided into several tiles. This is because the layers are encoded as Float and exceed 4 GB in size before GeoTIFF compression. All of these files need to be downloaded and located at Geodata/S2indices/RAW. The following R commands combine them, ensuring the coordinate systems and its naming, and pixels match the reference raster, while renaming files to EO_[index_]-[term: ST or LY][statistic]. Code # libs ---- if(!require(terra)) {install.packages(&quot;terra&quot;); require(terra)} if(!require(tidyverse)) {install.packages(&quot;tidyverse&quot;); require(tidyverse)} # 10 m raster template ---- template10=rast(&quot;./Templates/TemplateRasters/LV10m_10km.tif&quot;) # Fails as exported from GEE ---- faili=data.frame(fails=list.files(&quot;./Geodata/S2indices/RAW/&quot;)) faili$celi_sakums=paste0(&quot;./Geodata/S2indices/RAW/&quot;,faili$fails) faili=faili[-1,] # file names ---- faili=faili %&gt;% separate(fails,into=c(&quot;nosaukums&quot;,&quot;vidus&quot;,&quot;beigas&quot;),sep=&quot;-&quot;,remove = FALSE) %&gt;% mutate(mosaic_name=paste0(&quot;EO_&quot;,nosaukums,&quot;-&quot;,beigas,tolower(vidus),&quot;.tif&quot;), masaic_cels=paste0(&quot;./Geodata/S2indices/Mosaics/&quot;,mosaic_name)) unikalie=levels(factor(faili$mosaic_name)) min(table(faili$mosaic_name)) max(table(faili$mosaic_name)) # preparation of mosaics ---- for(i in seq_along(unikalie)){ sakums=Sys.time() unikalais=faili %&gt;% filter(mosaic_name==unikalie[i]) beigu_cels=unique(unikalais$masaic_cels) print(i) # there are exactly 2 tiles per file viens=rast(unikalais$celi_sakums[1]) divi=rast(unikalais$celi_sakums[2]) viens2=terra::project(viens,template10) divi2=terra::project(divi,template10) mozaika=terra::merge(viens2,divi2) maskets=mask(mozaika,template10, filename=beigu_cels,overwrite=TRUE, gdal=c(&quot;COMPRESS=LZW&quot;,&quot;TILED=YES&quot;,&quot;BIGTIFF=IF_SAFER&quot;), datatype=&quot;FLT4S&quot;, NAflag=NA) plot(maskets,main=unikalie[i]) print(beigu_cels) beigas=Sys.time() ilgums=beigas-sakums print(ilgums) } 4.14 Waste and garbage disposal sites, landfills Information on landfills has been compiled from VARAM and Latvian Environment, Geology and Meteorology Center “Report on landfills in Latvia in 2023” listed landfills and their addresses. The coordinates required for the preparation of EGVs were found by combining the resources https://www.google.com/maps and https://balticmaps.eu/. In addition to the resources mentioned above, an object was added at the address “Dardedzes C, Mārupes pag., Mārupes nov., Latvia, LV-2166”. In addition, information from the State Environmental Service on separated waste and deposit packaging collection points was used, exporting it to an Excel file. Both data sets were combined into a single file and added to this material. 4.15 Digital elevation/terrain models With the publication of continuous aerial laser scanning data for the territory of Latvia (https://www.lgia.gov.lv/lv/digitalie-augstuma-modeli-0), various high-resolution (1 m and higher) digital surface models (DSM) and digital elevation models (DEM) have been developed. Since the input data is the same in all cases, the values of these (corresponding) models are identical across almost the entire territory of the country. However, airborne laser scanning data (1) is not available for the entire territory of the country, and (2) there are differences between the models in terms of filling (availability of values) outside inland waters and (3) filling of water bodies themselves. However, for areas covered by data on land, the values are almost identical (Pearson’s correlation coefficients between the DEMs developed by LU ĢZZF, LVMI Silava, and LĢIA are greater than 0.999999). The arithmetic mean between the DEMs developed by LU ĢZZF and LVMI Silava, prepared in the LU project “Improvement of sustainable soil resource management in agriculture,” was used as the base DEM. The resolution of this DEM is 1 m, which is not necessary for species distribution modeling input data, therefore the layer is designed to correspond to the reference 10 m raster. When comparing the projected DEM with the reference, there are clearly distinguishable areas where there is no data. This has been solved by using the DEM with a resolution of 10 m developed by Māris Nartišs (LU ĢZZF) in 2018, which covers the entire territory of Latvia without gaps. To prevent sharp edges from forming in the fill areas (smooth transitions), an arithmetic mean layer was created, covering the entire territory of Latvia and matching the reference raster. A slope layer has also been created from this raster, which is designed in accordance with the reference. The slope is expressed in degrees and calculated using the 8-neighbor approach. The same applies to the aspect or slope direction. 4.16 Latvian Exclusive Economic Zone polygon The waters of Latvia’s Exclusive Economic Zone were obtained from the HELCOM map and data service. After downloading, this line file was analogically connected to the coastline file obtained from the same resource. 4.17 Nature Conservation Agency’s data vai tiešām izmantoju? References Domisch, S., Amatulli, G., Jetz, W., 2015. Near-global freshwater-specific environmental variables for biodiversity analyses in 1 km resolution. Scientific Data 2:150073, 1–13. https://doi.org/10.1038/sdata.2015.73 Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., 2017. Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sensing of Environment 202, 18–27. https://doi.org/10.1016/j.rse.2017.06.031 Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, H.P., Kessler, M., 2017. Data Descriptor: Climatologies at high resolution for the earth’s land surface areas. Scientific Data 4:170122. https://doi.org/10.1038/sdata.2017.122 "],["Ch05.html", "5 Geodata products 5.1 Terrain products 5.2 Soil products 5.3 Landscape classification 5.4 Landscape diversity 5.5 Forest diversity 5.6 Farmland diversity 5.7 Landscape in general diversity", " 5 Geodata products some raw data need extensive processing before EGVs can be created 5.1 Terrain products krā 5.2 Soil products krā 5.3 Landscape classification krā 5.4 Landscape diversity krā 5.5 Forest diversity krā 5.6 Farmland diversity krā 5.7 Landscape in general diversity krā "],["Ch06.html", "6 Ecogeographical variables", " 6 Ecogeographical variables Creation procedures of every EGV. "],["Ch07.html", "7 Data accessibility", " 7 Data accessibility This chapter provides access to EGVs described in previous parts Domisch, S., Amatulli, G., Jetz, W., 2015. Near-global freshwater-specific environmental variables for biodiversity analyses in 1 km resolution. Scientific Data 2:150073, 1–13. https://doi.org/10.1038/sdata.2015.73 Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., 2017. Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sensing of Environment 202, 18–27. https://doi.org/10.1016/j.rse.2017.06.031 Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, H.P., Kessler, M., 2017. Data Descriptor: Climatologies at high resolution for the earth’s land surface areas. Scientific Data 4:170122. https://doi.org/10.1038/sdata.2017.122 "]]
